{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =  ['/home/abhirup/Downloads/CMaterdb/IsolatedTrain/', '/home/abhirup/Downloads/CMaterdb/IsolatedTest/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_label(im_path):\n",
    "    return int(im_path.split('/')[6])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "global params\n",
    "params = {'path' : path,\n",
    "          'batch_size' : 50,\n",
    "          'input_size': (32,32),   \n",
    "          'lrn_rate': 0.0001, \n",
    "          'epochs': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_paths(path):\n",
    "    path_train, path_test = path\n",
    "    path_train = shuffle(sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path_train) for file in files]))\n",
    "    path_test = shuffle(sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path_test) for file in files]))\n",
    "    return path_train, path_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    img = cv2.imread(path, 0)\n",
    "    if img is None: return (False, None)\n",
    "    img = cv2.resize(cv2.threshold(img, 240, 1, cv2.THRESH_BINARY)[1], \n",
    "                     params['input_size'], \n",
    "                     cv2.INTER_AREA)\n",
    "    return np.expand_dims(img, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(label_data, num_class):\n",
    "    num_sample = np.shape(label_data)[0]\n",
    "    temp = np.zeros([num_sample, num_class])\n",
    "    temp[np.arange(num_sample),label_data] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph(model_dir = './logs_dir_bangla_recov_weights/'): \n",
    "    ckpt_file_path = model_dir + [i for i in os.listdir(model_dir) if i.endswith('meta')][0] \n",
    "    loader = tf.train.import_meta_graph(ckpt_file_path) \n",
    "    loader.restore(sess, tf.train.latest_checkpoint(model_dir)) \n",
    "    graph = tf.get_default_graph() \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs_dir_bangla_recov_weights/model.ckpt-200\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph() \n",
    "global graph  \n",
    "sess = tf.Session() \n",
    "graph= get_graph(model_dir= './logs_dir_bangla_recov_weights/')\n",
    "\n",
    "global c1_1w, c1_1b, c1_2w, c1_2b, c1_3w, c1_3b, c2_1w, c2_1b, c2_2w, c2_2b, c2_3w, c2_3b\n",
    "Saved_Tensors = [graph.get_tensor_by_name('encoder/Save_C1_1/kernel:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C1_1/bias:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C1_2/kernel:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C1_2/bias:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C1_3/kernel:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C1_3/bias:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C2_1/kernel:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C2_1/bias:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C2_2/kernel:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C2_2/bias:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C2_3/kernel:0'),\n",
    "                 graph.get_tensor_by_name('encoder/Save_C2_3/bias:0'),\n",
    "                ]\n",
    "Saved_Tensors = sess.run(Saved_Tensors)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c1_1w = tf.constant_initializer(Saved_Tensors[0])\n",
    "c1_1b = tf.constant_initializer(Saved_Tensors[1])\n",
    "c1_2w = tf.constant_initializer(Saved_Tensors[2])\n",
    "c1_2b = tf.constant_initializer(Saved_Tensors[3])\n",
    "c1_3w = tf.constant_initializer(Saved_Tensors[4])\n",
    "c1_3b = tf.constant_initializer(Saved_Tensors[5])\n",
    "c2_1w = tf.constant_initializer(Saved_Tensors[6])\n",
    "c2_1b = tf.constant_initializer(Saved_Tensors[7])\n",
    "c2_2w = tf.constant_initializer(Saved_Tensors[8])\n",
    "c2_2b = tf.constant_initializer(Saved_Tensors[9])\n",
    "c2_3w = tf.constant_initializer(Saved_Tensors[10])\n",
    "c2_3b = tf.constant_initializer(Saved_Tensors[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Network(Input): #input : [Batch_size, 32, 32, 1]\n",
    "    with tf.name_scope(\"Network\"):\n",
    "        conv1 = tf.layers.conv2d(Input, filters = 32, kernel_size = 3, strides = 1,  kernel_initializer=c1_1w, bias_initializer=c1_1b, padding='same' ,activation = tf.nn.relu, name = 'conv1')\n",
    "        conv2 = tf.layers.conv2d(conv1, filters = 32, kernel_size = 3, strides = 1,  kernel_initializer=c1_2w, bias_initializer=c1_2b, padding='same' ,activation = tf.nn.relu, name = 'conv2')\n",
    "        conv3 = tf.layers.conv2d(conv2, filters = 32, kernel_size = 3, strides = 1,  kernel_initializer=c1_3w, bias_initializer=c1_3b, padding='same' ,activation = tf.nn.relu, name = 'conv3')       \n",
    "        pool1 = tf.layers.max_pooling2d(conv3, pool_size = 2, strides = 2, name = 'pool1')       \n",
    "        conv4 = tf.layers.conv2d(pool1, filters = 64, kernel_size = 3, strides = 1,  kernel_initializer=c2_1w, bias_initializer=c2_1b, padding='same' ,activation = tf.nn.relu, name = 'conv4')       \n",
    "        conv5 = tf.layers.conv2d(conv4, filters = 64, kernel_size = 3, strides = 1,  kernel_initializer=c2_2w, bias_initializer=c2_2b, padding='same' ,activation = tf.nn.relu, name = 'conv5')       \n",
    "        conv6 = tf.layers.conv2d(conv5, filters = 64, kernel_size = 3, strides = 1,  kernel_initializer=c2_3w, bias_initializer=c2_3b, padding='same' ,activation = tf.nn.relu, name = 'conv6')       \n",
    "        pool2 = tf.layers.max_pooling2d(conv6, pool_size = 2, strides = 2, name = 'pool2')       \n",
    "        conv7 = tf.layers.conv2d(pool2, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv7')       \n",
    "        conv8 = tf.layers.conv2d(conv7, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv8')       \n",
    "        conv9 = tf.layers.conv2d(conv8, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv9')       \n",
    "        conv10 = tf.layers.conv2d(conv9, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv10')       \n",
    "        conv11 = tf.layers.conv2d(conv10, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv11')       \n",
    "        flat = tf.contrib.layers.flatten(tf.layers.batch_normalization(conv11))       \n",
    "        fc1 = tf.layers.dense(flat, units = 512, activation = tf.nn.relu, name = 'fc1')       \n",
    "        fc2 = tf.layers.dense(fc1, units = 512, activation = tf.nn.relu, name = 'fc2')       \n",
    "        fc3 = tf.layers.dense(fc2, units = 171, activation = tf.nn.relu, name = 'fc3')\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(logit, Label):\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit, labels = Label), name='Loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Accuracy_Evaluate(prediction, Label):\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Label, 1), name='Corr_Pred')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='Accuracy')\n",
    "#     print(correct_pred, accuracy)\n",
    "    return correct_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classifier_Model():\n",
    "    \n",
    "    batchsize = params['batch_size']\n",
    "    Input = tf.placeholder(dtype = tf.float32, shape = [batchsize, 32, 32, 1])\n",
    "    Label = tf.placeholder(dtype = tf.float32, shape = [batchsize, 171])\n",
    "    \n",
    "    logit = Network(Input)\n",
    "    prediction = tf.nn.softmax(logit)\n",
    "    loss = loss_function(logit, Label)\n",
    "    correct_pred, accuracy = Accuracy_Evaluate(prediction, Label)\n",
    "    optimiz = tf.train.AdadeltaOptimizer(params['lrn_rate']).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    tf.summary.scalar('Loss_Value',loss)\n",
    "    tf.summary.scalar('Accuracy',accuracy)\n",
    "    \n",
    "    print('Setting up summary op...')\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    print('Setting train summary...')\n",
    "    train_summary_writer = tf.summary.FileWriter('./log_dir_bangla_shared_train', sess.graph)\n",
    "    \n",
    "    print('Setting test summary')\n",
    "    test_summary_writer = tf.summary.FileWriter('./log_dir_bangla_shared_test', sess.graph)\n",
    "    t_summary_writer = tf.summary.FileWriter('./log_dir_bangla_shared_t', sess.graph)\n",
    "    \n",
    "#     print(\"Setting up Saver...\") \n",
    "#     saver = tf.train.Saver() \n",
    "    #summary_writer = tf.summary.FileWriter('./logs_dir_Bangla_Error/', sess.graph) \n",
    "    #ckpt = tf.train.get_checkpoint_state('./logs_dir_Bangla_Error/') \n",
    "    \n",
    "    train_paths, test_paths = get_file_paths(params['path'])\n",
    "    dataset_size = len(train_paths)\n",
    "    n_batches = dataset_size//batchsize\n",
    "         \n",
    "    itr = 1\n",
    "    for epoch in range(params['epochs']):\n",
    "         \n",
    "        for idx in range(n_batches): \n",
    "            \n",
    "            batch_paths = train_paths[idx * batchsize: (idx+1) * batchsize]\n",
    "            batch_data = np.array([load_data(path) for path in batch_paths])\n",
    "            batch_label = to_one_hot(np.array([load_label(path) for path in batch_paths]), 171)\n",
    "            \n",
    "            feed_dict = {Input : batch_data , Label : batch_label}\n",
    "            \n",
    "            _, train_loss, train_accuracy, train_summary_str = sess.run([optimiz, loss, accuracy, summary_op] , feed_dict )\n",
    "            train_summary_writer.add_summary(train_summary_str, itr)\n",
    "            itr = itr + 1\n",
    "            \n",
    "            if idx%10 == 0:\n",
    "                \n",
    "                print ('epoch : '+str(epoch)+' step : '+str(idx) + ' train_loss : '+str(train_loss) +\n",
    "                        ' train_accuracy : '+str(train_accuracy) \n",
    "                        )\n",
    "                \n",
    "            if idx%100 == 0:\n",
    "                test_batch_paths = np.array(test_paths)[np.random.choice(len(test_paths), batchsize)]\n",
    "                test_batch_data = np.array([load_data(path) for path in test_batch_paths])\n",
    "                test_batch_label = to_one_hot(np.array([load_label(path) for path in test_batch_paths]), 171)\n",
    "                feed_dict = {Input: test_batch_data, Label: test_batch_label}\n",
    "\n",
    "                test_accuracy, test_loss, test_summary_str = sess.run([accuracy, loss, summary_op], \n",
    "                                               feed_dict=feed_dict)\n",
    "                test_summary_writer.add_summary(test_summary_str, itr)\n",
    "                \n",
    "                print('epoch : ' +str(epoch)+' step : '+str(idx)+' test_loss : '+str(test_loss)+\n",
    "                      ' test_accuracy : '+str(test_accuracy))\n",
    "        \n",
    "        if epoch%40 == 0:\n",
    "            for idx in range(n_batches):\n",
    "                t_batch_data = np.array([load_data(path) for path in test_paths])\n",
    "                t_batch_label = np.array([load_label(path) for path in test_paths])\n",
    "                t_accuracy, t_loss, t_summary_str = sess.run([accuracy, loss, summary_op], \n",
    "                                               feed_dict=feed_dict)\n",
    "                t_summary_writer.add_summary(t_summary_str, itr)\n",
    "                \n",
    "                print('epoch : ' +str(epoch)+' step : '+str(idx)+' t_loss : '+str(t_loss)+\n",
    "                      ' t_accuracy : '+str(t_accuracy))\n",
    "            #if itr %100 == 0:\n",
    "             #   saver.save(sess, './logs_dir_Bengali_Error/' + \"model.ckpt\", itr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config = config)\n",
    "graph = tf.get_default_graph()\n",
    "# params['lrn_rate'] = 0.0009\n",
    "Classifier_Model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
