{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  ['/home/ayon/Database/IsolatedTrain/', '/home/ayon/Database/IsolatedTest/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global params\n",
    "params = {'path' : path,\n",
    "          'batch_size' : 32,\n",
    "          'input_size': (32,32), \n",
    "          'lrn_rate': 0.0008, \n",
    "          'epochs': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(path):\n",
    "    path_train, path_test = path\n",
    "    #path_train = shuffle(sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path_train) for file in files]))\n",
    "    path_train = sorted([os.path.join(root, file) for root, dirs, files in os.walk(path_train) for file in files])\n",
    "\n",
    "    path_test = shuffle(sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path_test) for file in files]), random_state = 0)\n",
    "    return path_train, path_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    img = cv2.imread(path, 0)\n",
    "    if img is None: return (False, None)\n",
    "    img = cv2.resize(img, \n",
    "                     params['input_size'], \n",
    "                     cv2.INTER_AREA)\n",
    "    return np.expand_dims(img, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label(im_path):\n",
    "    return int(im_path.split('/')[5])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(label_data, num_class):\n",
    "    num_sample = np.shape(label_data)[0]\n",
    "    temp = np.zeros([num_sample, num_class])\n",
    "    temp[np.arange(num_sample),label_data] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network(Input): #input : [Batch_size, 32, 32, 1]\n",
    "    with tf.name_scope(\"Network\"):\n",
    "        conv1 = tf.layers.conv2d(Input, filters = 32, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv1')\n",
    "        conv2 = tf.layers.conv2d(conv1, filters = 32, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv2')        \n",
    "        conv3 = tf.layers.conv2d(conv2, filters = 32, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv3')\n",
    "        pool1 = tf.layers.max_pooling2d(conv3, pool_size = 2, strides = 2, name = 'pool1')\n",
    "        conv4 = tf.layers.conv2d(pool1, filters = 64, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv4')\n",
    "        conv5 = tf.layers.conv2d(conv4, filters = 64, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv5')\n",
    "        conv6 = tf.layers.conv2d(conv5, filters = 64, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv6')\n",
    "        pool2 = tf.layers.max_pooling2d(conv6, pool_size = 2, strides = 2, name = 'pool2')\n",
    "        conv7 = tf.layers.conv2d(pool2, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv7')\n",
    "        conv8 = tf.layers.conv2d(conv7, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv8')\n",
    "        conv9 = tf.layers.conv2d(conv8, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv9')\n",
    "        conv10 = tf.layers.conv2d(conv9, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv10')\n",
    "        conv11 = tf.layers.conv2d(conv10, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv11')\n",
    "      \n",
    "#         flat = tf.contrib.layers.flatten(conv11)\n",
    "        flat = tf.contrib.layers.flatten(tf.contrib.layers.batch_norm(conv11))\n",
    "        fc1 = tf.layers.dense(flat, units = 512, activation = tf.nn.relu, name = 'fc1')\n",
    "        fc2 = tf.layers.dense(fc1, units = 512, activation = tf.nn.relu, name = 'fc2')   \n",
    "        fc3 = tf.layers.dense(fc2, units = 171, activation = None, name = 'fc3')\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def Network(Input): #input : [Batch_size, 32, 32, 1]\n",
    "    with tf.name_scope(\"Network\"):\n",
    "        conv1 = tf.layers.conv2d(Input, filters = 64, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv1')\n",
    "        conv2 = tf.layers.conv2d(conv1, filters = 64, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv2')\n",
    "        pool1 = tf.layers.max_pooling2d(conv2, pool_size = 2, strides = 2, name = 'pool1')\n",
    "\n",
    "        conv3 = tf.layers.conv2d(pool1, filters = 256, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv3')\n",
    "        conv4 = tf.layers.conv2d(conv3, filters = 256, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv4')\n",
    "        pool2 = tf.layers.max_pooling2d(conv4, pool_size = 2, strides = 2, name = 'pool2')\n",
    "\n",
    "        conv5 = tf.layers.conv2d(pool2, filters = 512, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv5')\n",
    "\n",
    "        flat = tf.contrib.layers.flatten(tf.contrib.layers.batch_norm(conv5))\n",
    "        fc1 = tf.layers.dense(flat, units = 1024, activation = tf.nn.relu, name = 'fc1')\n",
    "        fc2 = tf.layers.dense(fc1, units = 171, activation = None, name = 'fc2')\n",
    "    \n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logit, Label):\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit, labels = Label), name='Loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy_Evaluate(prediction, Label):\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Label, 1), name='Corr_Pred')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='Accuracy')\n",
    "    return correct_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier():\n",
    "    \n",
    "    batchsize = params['batch_size']\n",
    "    Input = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32, 1])\n",
    "    Label = tf.placeholder(dtype = tf.float32, shape = [None, 171])\n",
    "    \n",
    "    logit = Network(Input)\n",
    "    \n",
    "    prediction = tf.nn.softmax(logit)\n",
    "    \n",
    "    loss = loss_function(logit, Label)\n",
    "    \n",
    "    correct_pred, accuracy = Accuracy_Evaluate(prediction, Label)\n",
    "    \n",
    "    \n",
    "    optimiz = tf.train.GradientDescentOptimizer(params['lrn_rate']).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    " \n",
    "    \n",
    "    tf.summary.scalar('Loss_Value',loss)\n",
    "    tf.summary.scalar('Accuracy',accuracy)\n",
    "    \n",
    "    print('Setting up summary op...')\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    print('Setting up train summary...')\n",
    "    train_summary_writer = tf.summary.FileWriter('./log_dir/train', sess.graph)\n",
    "    \n",
    "    print('Setting up test summary....')\n",
    "    test_summary_writer = tf.summary.FileWriter('./log_dir/test', sess.graph)\n",
    "    t_summary_writer = tf.summary.FileWriter('./log_dir/t', sess.graph)\n",
    "       \n",
    "    train_paths, test_paths = get_file_paths(params['path'])\n",
    "    dataset_size = len(train_paths)\n",
    "    n_batches = dataset_size//batchsize\n",
    "    \n",
    "    itr = 1\n",
    "    for epoch in range(params['epochs']):\n",
    "         \n",
    "        for idx in range(n_batches): \n",
    "            \n",
    "            batch_paths = train_paths[idx * batchsize: (idx+1) * batchsize]\n",
    "            batch_data = np.array([load_data(path) for path in batch_paths])\n",
    "            batch_label = to_one_hot(np.array([load_label(path) for path in batch_paths]), 171)\n",
    "            \n",
    "            feed_dict = {Input : batch_data , Label : batch_label}\n",
    "            \n",
    "            _, train_loss, train_accuracy, train_summary_str = sess.run([optimiz, loss, accuracy, summary_op] , feed_dict )\n",
    "            train_summary_writer.add_summary(train_summary_str, itr)\n",
    "            itr = itr + 1\n",
    "            \n",
    "            if idx%10 == 0:\n",
    "                \n",
    "                print ('epoch : '+str(epoch)+' step : '+str(idx) + ' train_loss : '+str(train_loss) +\n",
    "                        ' train_accuracy : '+str(train_accuracy) \n",
    "                        )\n",
    "                \n",
    "#             if idx%100 == 0:\n",
    "#                 test_batch_paths = np.array(test_paths)[np.random.choice(len(test_paths), batchsize)]\n",
    "#                 test_batch_data = np.array([load_data(path) for path in test_batch_paths])\n",
    "#                 test_batch_label = to_one_hot(np.array([load_label(path) for path in test_batch_paths]), 171)\n",
    "#                 feed_dict = {Input: test_batch_data, Label: test_batch_label}\n",
    "\n",
    "#                 test_accuracy, test_loss, test_summary_str = sess.run([accuracy, loss, summary_op], \n",
    "#                                                feed_dict=feed_dict)\n",
    "#                 test_summary_writer.add_summary(test_summary_str, itr)\n",
    "                \n",
    "#                 print('epoch : ' +str(epoch)+' step : '+str(idx)+' test_loss : '+str(test_loss)+' test_accuracy : '+str(test_accuracy))\n",
    "        \n",
    "        if epoch%40 == 0:\n",
    "            for idx in range(n_batches):\n",
    "                t_batch_paths = test_paths[:]\n",
    "                t_batch_data = np.array([load_data(path) for path in t_batch_paths])\n",
    "                t_batch_label = to_one_hot(np.array([load_label(path) for path in t_batch_paths]))\n",
    "                feed_dictr = {Input: t_batch_data, Label: t_batch_label}\n",
    "                t_accuracy, t_loss, t_summary_str = sess.run([accuracy, loss, summary_op], \n",
    "                                               feed_dict=feed_dict)\n",
    "                t_summary_writer.add_summary(t_summary_str, itr)\n",
    "                \n",
    "                print('epoch : ' +str(epoch)+' step : '+str(idx)+' t_loss : '+str(t_loss)+' t_accuracy : '+str(t_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up summary op...\n",
      "Setting up train summary...\n",
      "Setting up test summary....\n",
      "epoch : 0 step : 0 train_loss : 0.773995 train_accuracy : 0.0\n",
      "epoch : 0 step : 10 train_loss : 0.658848 train_accuracy : 0.0\n",
      "epoch : 0 step : 20 train_loss : 0.610479 train_accuracy : 0.0\n",
      "epoch : 0 step : 30 train_loss : 0.575674 train_accuracy : 0.0\n",
      "epoch : 0 step : 40 train_loss : 0.539463 train_accuracy : 0.0\n",
      "epoch : 0 step : 50 train_loss : 0.526127 train_accuracy : 0.0\n",
      "epoch : 0 step : 60 train_loss : 0.487664 train_accuracy : 0.71875\n",
      "epoch : 0 step : 70 train_loss : 0.472363 train_accuracy : 0.0\n",
      "epoch : 0 step : 80 train_loss : 0.454507 train_accuracy : 0.0\n",
      "epoch : 0 step : 90 train_loss : 0.444224 train_accuracy : 0.0\n",
      "epoch : 0 step : 100 train_loss : 0.428503 train_accuracy : 0.0\n",
      "epoch : 0 step : 110 train_loss : 0.420993 train_accuracy : 0.0\n",
      "epoch : 0 step : 120 train_loss : 0.39704 train_accuracy : 0.0\n",
      "epoch : 0 step : 130 train_loss : 0.387543 train_accuracy : 0.0\n",
      "epoch : 0 step : 140 train_loss : 0.376533 train_accuracy : 0.0\n",
      "epoch : 0 step : 150 train_loss : 0.359119 train_accuracy : 0.0\n",
      "epoch : 0 step : 160 train_loss : 0.348896 train_accuracy : 0.0\n",
      "epoch : 0 step : 170 train_loss : 0.346893 train_accuracy : 0.0\n",
      "epoch : 0 step : 180 train_loss : 0.332961 train_accuracy : 0.0\n",
      "epoch : 0 step : 190 train_loss : 0.31135 train_accuracy : 0.0\n",
      "epoch : 0 step : 200 train_loss : 0.297471 train_accuracy : 0.0\n",
      "epoch : 0 step : 210 train_loss : 0.295435 train_accuracy : 0.0\n",
      "epoch : 0 step : 220 train_loss : 0.299258 train_accuracy : 0.0\n",
      "epoch : 0 step : 230 train_loss : 0.277525 train_accuracy : 0.0\n",
      "epoch : 0 step : 240 train_loss : 0.263189 train_accuracy : 0.0\n",
      "epoch : 0 step : 250 train_loss : 0.254347 train_accuracy : 0.03125\n",
      "epoch : 0 step : 260 train_loss : 0.257749 train_accuracy : 0.0\n",
      "epoch : 0 step : 270 train_loss : 0.244378 train_accuracy : 0.0\n",
      "epoch : 0 step : 280 train_loss : 0.242715 train_accuracy : 0.0\n",
      "epoch : 0 step : 290 train_loss : 0.232675 train_accuracy : 0.0\n",
      "epoch : 0 step : 300 train_loss : 0.226814 train_accuracy : 0.0\n",
      "epoch : 0 step : 310 train_loss : 0.218093 train_accuracy : 0.0\n",
      "epoch : 0 step : 320 train_loss : 0.205571 train_accuracy : 0.0\n",
      "epoch : 0 step : 330 train_loss : 0.212035 train_accuracy : 0.0\n",
      "epoch : 0 step : 340 train_loss : 0.218502 train_accuracy : 0.0\n",
      "epoch : 0 step : 350 train_loss : 0.18522 train_accuracy : 0.0\n",
      "epoch : 0 step : 360 train_loss : 0.189367 train_accuracy : 0.0\n",
      "epoch : 0 step : 370 train_loss : 0.182509 train_accuracy : 0.0\n",
      "epoch : 0 step : 380 train_loss : 0.166521 train_accuracy : 0.0\n",
      "epoch : 0 step : 390 train_loss : 0.166976 train_accuracy : 0.0\n",
      "epoch : 0 step : 400 train_loss : 0.158383 train_accuracy : 0.0\n",
      "epoch : 0 step : 410 train_loss : 0.158808 train_accuracy : 0.0\n",
      "epoch : 0 step : 420 train_loss : 0.156087 train_accuracy : 0.0\n",
      "epoch : 0 step : 430 train_loss : 0.146224 train_accuracy : 0.0\n",
      "epoch : 0 step : 440 train_loss : 0.143867 train_accuracy : 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d51602dda583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-713a7f0bc00a>\u001b[0m in \u001b[0;36mclassifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mInput\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mLabel\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimiz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_summary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mitr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config = config)\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "classifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
