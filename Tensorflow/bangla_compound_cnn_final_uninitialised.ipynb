{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  ['/home/ayon/Database/IsolatedTrain/', '/home/ayon/Database/IsolatedTest/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global params\n",
    "params = {'path' : path,\n",
    "          'batch_size' : 50,\n",
    "          'input_size': (32,32), \n",
    "          'lrn_rate': 0.0001, \n",
    "          'epochs': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(path):\n",
    "    path_train, path_test = path\n",
    "    path_train = shuffle(sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path_train) for file in files]))\n",
    "    path_test = shuffle(sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path_test) for file in files]))\n",
    "    return path_train, path_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    img = cv2.imread(path, 0)\n",
    "    if img is None: return (False, None)\n",
    "    img = cv2.resize(cv2.threshold(img, 240, 1, cv2.THRESH_BINARY)[1], \n",
    "                     params['input_size'], \n",
    "                     cv2.INTER_AREA)\n",
    "    return np.expand_dims(img, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label(im_path):\n",
    "    return int(im_path.split('/')[5])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(label_data, num_class):\n",
    "    num_sample = np.shape(label_data)[0]\n",
    "    temp = np.zeros([num_sample, num_class])\n",
    "    temp[np.arange(num_sample),label_data] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network(Input): #input : [Batch_size, 32, 32, 1]\n",
    "    with tf.name_scope(\"Network\"):\n",
    "        conv1 = tf.layers.conv2d(Input, filters = 32, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv1')\n",
    "        conv2 = tf.layers.conv2d(conv1, filters = 32, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv2')        \n",
    "        conv3 = tf.layers.conv2d(conv2, filters = 32, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv3')\n",
    "        pool1 = tf.layers.max_pooling2d(conv3, pool_size = 2, strides = 2, name = 'pool1')\n",
    "        conv4 = tf.layers.conv2d(pool1, filters = 64, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv4')\n",
    "        conv5 = tf.layers.conv2d(conv4, filters = 64, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv5')\n",
    "        conv6 = tf.layers.conv2d(conv5, filters = 64, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv6')\n",
    "        pool2 = tf.layers.max_pooling2d(conv6, pool_size = 2, strides = 2, name = 'pool2')\n",
    "        conv7 = tf.layers.conv2d(pool2, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv7')\n",
    "        conv8 = tf.layers.conv2d(conv7, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv8')\n",
    "        conv9 = tf.layers.conv2d(conv8, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv9')\n",
    "        conv10 = tf.layers.conv2d(conv9, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv10')\n",
    "        conv11 = tf.layers.conv2d(conv10, filters = 128, kernel_size = 3, strides = 1,  padding='same' ,activation = tf.nn.relu, name = 'conv11')\n",
    "      \n",
    "        flat = tf.contrib.layers.flatten(tf.layers.batch_normalization(conv11))\n",
    "        fc1 = tf.layers.dense(flat, units = 512, activation = tf.nn.relu, name = 'fc1')\n",
    "        fc2 = tf.layers.dense(fc1, units = 512, activation = tf.nn.relu, name = 'fc2')   \n",
    "        fc3 = tf.layers.dense(fc2, units = 171, activation = None, name = 'fc3')\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logit, Label):\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit, labels = Label), name='Loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy_Evaluate(prediction, Label):\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Label, 1), name='Corr_Pred')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='Accuracy')\n",
    "    return correct_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier():\n",
    "    \n",
    "    batchsize = params['batch_size']\n",
    "    Input = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32, 1])\n",
    "    Label = tf.placeholder(dtype = tf.float32, shape = [None, 171])\n",
    "    \n",
    "    logit = Network(Input)\n",
    "    \n",
    "    prediction = tf.nn.softmax(logit)\n",
    "    \n",
    "    loss = loss_function(logit, Label)\n",
    "    \n",
    "    correct_pred, accuracy = Accuracy_Evaluate(prediction, Label)\n",
    "    \n",
    "    \n",
    "    optimiz = tf.train.AdadeltaOptimizer(params['lrn_rate']).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    " \n",
    "    \n",
    "    tf.summary.scalar('Loss_Value',loss)\n",
    "    tf.summary.scalar('Accuracy',accuracy)\n",
    "    \n",
    "    print('Setting up summary op...')\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    print('Setting up train summary...')\n",
    "    train_summary_writer = tf.summary.FileWriter('./log_dir/train', sess.graph)\n",
    "    \n",
    "    print('Setting up test summary....')\n",
    "    test_summary_writer = tf.summary.FileWriter('./log_dir/test', sess.graph)\n",
    "    t_summary_writer = tf.summary.FileWriter('./log_dir/t', sess.graph)\n",
    "       \n",
    "    train_paths, test_paths = get_file_paths(params['path'])\n",
    "    dataset_size = len(train_paths)\n",
    "    n_batches = dataset_size//batchsize\n",
    "    \n",
    "    itr = 1\n",
    "    for epoch in range(params['epochs']):\n",
    "         \n",
    "        for idx in range(n_batches): \n",
    "            \n",
    "            batch_paths = train_paths[idx * batchsize: (idx+1) * batchsize]\n",
    "            batch_data = np.array([load_data(path) for path in batch_paths])\n",
    "            batch_label = to_one_hot(np.array([load_label(path) for path in batch_paths]), 171)\n",
    "            \n",
    "            feed_dict = {Input : batch_data , Label : batch_label}\n",
    "            \n",
    "            _, train_loss, train_accuracy, train_summary_str = sess.run([optimiz, loss, accuracy, summary_op] , feed_dict )\n",
    "            train_summary_writer.add_summary(train_summary_str, itr)\n",
    "            itr = itr + 1\n",
    "            \n",
    "            if idx%10 == 0:\n",
    "                \n",
    "                print ('epoch : '+str(epoch)+' step : '+str(idx) + ' train_loss : '+str(train_loss) +\n",
    "                        ' train_accuracy : '+str(train_accuracy) \n",
    "                        )\n",
    "                \n",
    "            if idx%100 == 0:\n",
    "                test_batch_paths = np.array(test_paths)[np.random.choice(len(test_paths), batchsize)]\n",
    "                test_batch_data = np.array([load_data(path) for path in test_batch_paths])\n",
    "                test_batch_label = to_one_hot(np.array([load_label(path) for path in test_batch_paths]), 171)\n",
    "                feed_dict = {Input: test_batch_data, Label: test_batch_label}\n",
    "\n",
    "                test_accuracy, test_loss, test_summary_str = sess.run([accuracy, loss, summary_op], \n",
    "                                               feed_dict=feed_dict)\n",
    "                test_summary_writer.add_summary(test_summary_str, itr)\n",
    "                \n",
    "                print('epoch : ' +str(epoch)+' step : '+str(idx)+' test_loss : '+str(test_loss)+' test_accuracy : '+str(test_accuracy))\n",
    "        \n",
    "        if epoch%40 == 0:\n",
    "            for idx in range(n_batches):\n",
    "                t_batch_paths = test_paths[:]\n",
    "                t_batch_data = np.array([load_data(path) for path in t_batch_paths])\n",
    "                t_batch_label = np.array([load_label(path) for path in t_batch_paths])\n",
    "                t_accuracy, t_loss, t_summary_str = sess.run([accuracy, loss, summary_op], \n",
    "                                               feed_dict=feed_dict)\n",
    "                t_summary_writer.add_summary(t_summary_str, itr)\n",
    "                \n",
    "                print('epoch : ' +str(epoch)+' step : '+str(idx)+' t_loss : '+str(t_loss)+' t_accuracy : '+str(t_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config = config)\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "classifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
