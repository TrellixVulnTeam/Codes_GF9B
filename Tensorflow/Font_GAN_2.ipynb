{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from ops import *\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './Font_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global params\n",
    "\n",
    "params = {'path' : path,\n",
    "          'batch_size' : 32,\n",
    "          'output_size': 256,\n",
    "          'gf_dim': 32,\n",
    "          'df_dim': 32,\n",
    "          'L1_lambda': 100,\n",
    "          'lr': 0.0001,\n",
    "          'beta1': 0.5,\n",
    "          'epochs': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_paths(path):\n",
    "    paths = sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path) for file in files])\n",
    "    return paths[:int(len(paths)*.8)], paths[int(len(paths)*.8):]  #trainpaths, testpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):   \n",
    "    img = np.expand_dims(cv2.imread(path,0),-1)\n",
    "    img_ = img[:,img.shape[1]//2-128:img.shape[1]//2+128]\n",
    "    if img_.shape[1]==256:\n",
    "        return img_/127.5 - 1.\n",
    "    else:\n",
    "        return np.expand_dims(cv2.resize(img_,(256,512)),-1)/127.5 - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(a[0][5,:,:,0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global d_bn1, d_bn2, d_bn3\n",
    "\n",
    "global g_bn_e2, g_bn_e3, g_bn_e4, g_bn_e5, g_bn_e6, g_bn_e7, g_bn_e8\n",
    "\n",
    "global g_bn_d1, g_bn_d2, g_bn_d3, g_bn_d4, g_bn_d5, g_bn_d6 , g_bn_d7\n",
    "\n",
    "d_bn1 = batch_norm(name='d_bn1')\n",
    "d_bn2 = batch_norm(name='d_bn2')\n",
    "d_bn3 = batch_norm(name='d_bn3')\n",
    "\n",
    "g_bn_e2 = batch_norm(name='g_bn_e2')\n",
    "g_bn_e3 = batch_norm(name='g_bn_e3')\n",
    "g_bn_e4 = batch_norm(name='g_bn_e4')\n",
    "g_bn_e5 = batch_norm(name='g_bn_e5')\n",
    "g_bn_e6 = batch_norm(name='g_bn_e6')\n",
    "g_bn_e7 = batch_norm(name='g_bn_e7')\n",
    "g_bn_e8 = batch_norm(name='g_bn_e8')\n",
    "\n",
    "g_bn_d1 = batch_norm(name='g_bn_d1')\n",
    "g_bn_d2 = batch_norm(name='g_bn_d2')\n",
    "g_bn_d3 = batch_norm(name='g_bn_d3')\n",
    "g_bn_d4 = batch_norm(name='g_bn_d4')\n",
    "g_bn_d5 = batch_norm(name='g_bn_d5')\n",
    "g_bn_d6 = batch_norm(name='g_bn_d6')\n",
    "g_bn_d7 = batch_norm(name='g_bn_d7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(image, y=None):\n",
    "    \n",
    "    s = params['output_size']\n",
    "    output_c_dim = 1\n",
    "    s2, s4, s8, s16, s32, s64, s128 = int(s/2), int(s/4), int(s/8), int(s/16), int(s/32), int(s/64), int(s/128)\n",
    "    gf_dim = params['gf_dim']\n",
    "        \n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "\n",
    "\n",
    "\n",
    "        # image is (256 x 256 x input_c_dim)\n",
    "        e1 = conv2d(image, gf_dim, name='g_e1_conv')\n",
    "        # e1 is (128 x 128 x gf_dim)\n",
    "        e2 = g_bn_e2(conv2d(lrelu(e1), gf_dim*2, name='g_e2_conv'))\n",
    "        # e2 is (64 x 64 x gf_dim*2)\n",
    "        e3 = g_bn_e3(conv2d(lrelu(e2), gf_dim*4, name='g_e3_conv'))\n",
    "        # e3 is (32 x 32 x gf_dim*4)\n",
    "        e4 = g_bn_e4(conv2d(lrelu(e3), gf_dim*8, name='g_e4_conv'))\n",
    "        # e4 is (16 x 16 x gf_dim*8)\n",
    "        e5 = g_bn_e5(conv2d(lrelu(e4), gf_dim*8, name='g_e5_conv'))\n",
    "        # e5 is (8 x 8 x gf_dim*8)\n",
    "        e6 = g_bn_e6(conv2d(lrelu(e5), gf_dim*8, name='g_e6_conv'))\n",
    "        # e6 is (4 x 4 x gf_dim*8)\n",
    "        e7 = g_bn_e7(conv2d(lrelu(e6), gf_dim*8, name='g_e7_conv'))\n",
    "        # e7 is (2 x 2 x gf_dim*8)\n",
    "        e8 = g_bn_e8(conv2d(lrelu(e7), gf_dim*8, name='g_e8_conv'))\n",
    "        # e8 is (1 x 1 x gf_dim*8)\n",
    "        \n",
    "        batch_size = params['batch_size']\n",
    "        d1, d1_w, d1_b = deconv2d(tf.nn.relu(e8),\n",
    "            [batch_size, s128, s128, gf_dim*8], name='g_d1', with_w=True)\n",
    "        d1 = tf.nn.dropout(g_bn_d1(d1), 0.5)\n",
    "        d1 = tf.concat([d1, e7], 3)\n",
    "        # d1 is (2 x 2 x gf_dim*8*2)\n",
    "\n",
    "        d2, d2_w, d2_b = deconv2d(tf.nn.relu(d1),\n",
    "            [batch_size, s64, s64, gf_dim*8], name='g_d2', with_w=True)\n",
    "        d2 = tf.nn.dropout(g_bn_d2(d2), 0.5)\n",
    "        d2 = tf.concat([d2, e6], 3)\n",
    "        # d2 is (4 x 4 x gf_dim*8*2)\n",
    "\n",
    "        d3, d3_w, d3_b = deconv2d(tf.nn.relu(d2),\n",
    "            [batch_size, s32, s32, gf_dim*8], name='g_d3', with_w=True)\n",
    "        d3 = tf.nn.dropout(g_bn_d3(d3), 0.5)\n",
    "        d3 = tf.concat([d3, e5], 3)\n",
    "        # d3 is (8 x 8 x gf_dim*8*2)\n",
    "\n",
    "        d4, d4_w, d4_b = deconv2d(tf.nn.relu(d3),\n",
    "            [batch_size, s16, s16, gf_dim*8], name='g_d4', with_w=True)\n",
    "        d4 = g_bn_d4(d4)\n",
    "        d4 = tf.concat([d4, e4], 3)\n",
    "        # d4 is (16 x 16 x gf_dim*8*2)\n",
    "\n",
    "        d5, d5_w, d5_b = deconv2d(tf.nn.relu(d4),\n",
    "            [batch_size, s8, s8, gf_dim*4], name='g_d5', with_w=True)\n",
    "        d5 = g_bn_d5(d5)\n",
    "        d5 = tf.concat([d5, e3], 3)\n",
    "        # d5 is (32 x 32 x gf_dim*4*2)\n",
    "\n",
    "        d6, d6_w, sd6_b = deconv2d(tf.nn.relu(d5),\n",
    "            [batch_size, s4, s4, gf_dim*2], name='g_d6', with_w=True)\n",
    "        d6 = g_bn_d6(d6)\n",
    "        d6 = tf.concat([d6, e2], 3)\n",
    "        # d6 is (64 x 64 x gf_dim*2*2)\n",
    "\n",
    "        d7, d7_w, d7_b = deconv2d(tf.nn.relu(d6),\n",
    "            [batch_size, s2, s2, gf_dim], name='g_d7', with_w=True)\n",
    "        d7 = g_bn_d7(d7)\n",
    "        d7 = tf.concat([d7, e1], 3)\n",
    "        # d7 is (128 x 128 x gf_dim*1*2)\n",
    "\n",
    "        d8, d8_w, d8_b = deconv2d(tf.nn.relu(d7),\n",
    "            [batch_size, s, s, output_c_dim], name='g_d8', with_w=True)\n",
    "        # d8 is (256 x 256 x output_c_dim)\n",
    "\n",
    "        #return tf.nn.tanh(d8[:,:,:,:3]), tf.nn.tanh(d8[:,:,:,3:4])  #(w/o text , bin text)\n",
    "        return tf.nn.tanh(d8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(image, y=None, reuse=False):\n",
    "    \n",
    "    df_dim = params['df_dim']\n",
    "    batch_size = params['batch_size']\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "\n",
    "        # image is 256 x 256 x (input_c_dim + output_c_dim)\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        else:\n",
    "            assert tf.get_variable_scope().reuse == False\n",
    "\n",
    "        h0 = lrelu(conv2d(image, df_dim, name='d_h0_conv'))\n",
    "        # h0 is (128 x 128 x df_dim)\n",
    "        h1 = lrelu(d_bn1(conv2d(h0, df_dim*2, name='d_h1_conv')))\n",
    "        # h1 is (64 x 64 x df_dim*2)\n",
    "        h2 = lrelu(d_bn2(conv2d(h1, df_dim*4, name='d_h2_conv')))\n",
    "        # h2 is (32x 32 x df_dim*4)\n",
    "        h3 = lrelu(d_bn3(conv2d(h2, df_dim*8, d_h=1, d_w=1, name='d_h3_conv')))\n",
    "        # h3 is (16 x 16 x df_dim*8)\n",
    "        h4 = linear(tf.reshape(h3, [batch_size, -1]), 1, 'd_h3_lin')\n",
    "\n",
    "        return tf.nn.sigmoid(h4), h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GAN_model():\n",
    "    \n",
    "    real_A = tf.placeholder(dtype=tf.float32, shape=[params['batch_size'],256,256,1], name = 'real_A') #Arial,real\n",
    "    real_B = tf.placeholder(dtype=tf.float32, shape=[params['batch_size'],256,256,1], name = 'real_B') #Times,real\n",
    "    \n",
    "    fake_B = generator(real_A) #Times,fake\n",
    "    \n",
    "    real_B_sum = tf.summary.image(\"Real_Times\", real_B, max_outputs=1) \n",
    "    fake_B_sum = tf.summary.image(\"Fake_times\", fake_B, max_outputs=1) \n",
    "    real_A_sum = tf.summary.image(\"Arial\", real_A, max_outputs=1)\n",
    "    tf.summary.histogram(\"Value_Range\", fake_B)\n",
    "    \n",
    "    \n",
    "    real_AB = tf.concat([real_A,real_B],3)\n",
    "    fake_AB = tf.concat([real_A,fake_B],3)\n",
    "    \n",
    "    D_real,D_real_logits = discriminator(real_AB, reuse=False)\n",
    "    D_fake,D_fake_logits = discriminator(fake_AB, reuse=True)\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake))) + params['L1_lambda'] * tf.reduce_mean(tf.abs(real_B - fake_B))\n",
    "    \n",
    "    g_loss_sum = tf.summary.scalar(\"g_loss\", g_loss)\n",
    "    d_loss_sum = tf.summary.scalar(\"d_loss\", d_loss)\n",
    "    \n",
    "    t_vars = tf.trainable_variables()\n",
    "\n",
    "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "    g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "    \n",
    "    \n",
    "    d_optim = tf.train.AdamOptimizer(0.0005).minimize(d_loss, var_list = d_vars)\n",
    "    g_optim = tf.train.AdamOptimizer(0.0005).minimize(g_loss, var_list = g_vars)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    summary = tf.summary.merge_all()\n",
    "    trainwriter = tf.summary.FileWriter(\"./logs3/train\", sess.graph)\n",
    "    testwriter = tf.summary.FileWriter(\"./logs3/test\", sess.graph)\n",
    "    \n",
    "    \n",
    "    counter = 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trainpaths, testpaths = get_file_paths(params['path'])\n",
    "    \n",
    "    data_set_size = len(trainpaths)\n",
    "    \n",
    "    \n",
    "    #sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    print (data_set_size//params['batch_size'])\n",
    "    for epoch in range(params['epochs']):\n",
    "        \n",
    "        for idx in range(data_set_size//params['batch_size']):\n",
    "            \n",
    "            batch_path = trainpaths[idx * params['batch_size'] : (idx + 1) * params['batch_size']]\n",
    "            batch_data = np.array([load_data(path) for path in batch_path])\n",
    "            print(batch_data.shape)\n",
    "            KK = batch_data[:,:256]\n",
    "            print(KK.shape)\n",
    "\n",
    "            feed_dict = {real_A : batch_data[:,:256] , real_B : batch_data[:,256:]}\n",
    "            \n",
    "            _, d_loss_ = sess.run([d_optim, d_loss], feed_dict) # update D network\n",
    "            _, g_loss_ = sess.run([g_optim, g_loss], feed_dict) # update G network\n",
    "            _,summary_, g_loss_ = sess.run([g_optim,summary, g_loss], feed_dict) # update G network\n",
    "            \n",
    "            trainwriter.add_summary(summary_, counter)\n",
    "            \n",
    "            print ('#epoch : ' +str(epoch) + ' idx : ' +str(idx) + ' Dis loss : '+str(d_loss_) + ' Gen loss : '+str(g_loss_))\n",
    "            \n",
    "            if idx%10==0:\n",
    "                \n",
    "                batch_path_test = np.array(testpaths)[np.random.choice(len(testpaths),32)]\n",
    "                batch_data_test = np.array([load_data(path) for path in batch_path_test])\n",
    "                feed_dict_test = {real_A : batch_data_test[:,:256] , real_B : batch_data_test[:,256:]}\n",
    "                test_summary_ = sess.run(summary, feed_dict_test)\n",
    "                testwriter.add_summary(test_summary_,counter)\n",
    "                _\n",
    "            counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "171*5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
